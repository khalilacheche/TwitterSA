{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from ekphrasis.dicts.noslang.slangdict import slangdict\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "\n",
    "#train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "#vectorize the text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499995</th>\n",
       "      <td>a warning sign ? (; rt &lt;user&gt; the negativity y...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499996</th>\n",
       "      <td>&lt;user&gt; ff too thank youuu ) )</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499997</th>\n",
       "      <td>i just love shumpa ! that's my girl</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499998</th>\n",
       "      <td>the best way to start a day ! no matter what h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499999</th>\n",
       "      <td>#frenchieswant1dtou i'm not from french but &lt;u...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label\n",
       "0        vinco tresorpack 6 ( difficulty 10 of 10 objec...   -1.0\n",
       "1        glad i dot have taks tomorrow ! ! #thankful #s...   -1.0\n",
       "2        1-3 vs celtics in the regular season = were fu...   -1.0\n",
       "3        <user> i could actually kill that girl i'm so ...   -1.0\n",
       "4        <user> <user> <user> i find that very hard to ...   -1.0\n",
       "...                                                    ...    ...\n",
       "2499995  a warning sign ? (; rt <user> the negativity y...    1.0\n",
       "2499996                      <user> ff too thank youuu ) )    1.0\n",
       "2499997                i just love shumpa ! that's my girl    1.0\n",
       "2499998  the best way to start a day ! no matter what h...    1.0\n",
       "2499999  #frenchieswant1dtou i'm not from french but <u...    1.0\n",
       "\n",
       "[2500000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/train_neg_full.txt\") as f:\n",
    "    df_train_neg = [l for l in f.read().split(\"\\n\") if len(l)>0]\n",
    "    df_train_neg = pd.DataFrame(df_train_neg,columns=[\"text\"])\n",
    "    df_train_neg[\"label\"] = -np.ones(df_train_neg.shape[0])\n",
    "with open(\"./data/train_pos_full.txt\") as f:\n",
    "    df_train_pos= [l for l in f.read().split(\"\\n\") if len(l)>0]\n",
    "    df_train_pos = pd.DataFrame(df_train_pos,columns=[\"text\"])\n",
    "    df_train_pos[\"label\"] = np.ones(df_train_pos.shape[0])\n",
    "df_train = pd.concat([df_train_neg,df_train_pos],axis=0,ignore_index=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test_data.txt', sep = '\\t', header = None)\n",
    "df_test.columns = ['text']\n",
    "#drop the strings before the first , on the test data\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x.split(',', 1)[1])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# change 1 \n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "def handle_repeating_char(text):\n",
    "    \"\"\"\n",
    "    Normalize to 2 repetitions of a single char.\n",
    "    When a char is repeated at least 2 times, keep only 2 repetitions.\n",
    "    e.g. \"goood\" becomes \"good\"\n",
    "    \"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "   # terms that will be normalized \n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user','time', 'url', 'date', 'number'],\n",
    "    # corpus from which the word statistics are going to be used for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    #list of dictionaries, for replacing tokens extracted from the text,\n",
    "    #with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "\n",
    "def clean_text(text, lemmatize = True, remove_stopwords = True, caseFolding = True, slang = True, double = True, text_cleaning = True):\n",
    "    \n",
    "    \n",
    "    if caseFolding:\n",
    "        text = text.lower()\n",
    "    text = tokenizer.tokenize(text)\n",
    "    if slang :\n",
    "        text = [(slangdict[w] if w in slangdict else w) for w in text]\n",
    "    if double:\n",
    "        text = [handle_repeating_char(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    if text_cleaning:\n",
    "        text = text_processor.pre_process_doc(text)\n",
    "    if remove_stopwords:\n",
    "        text = [word for word in text if word not in stop_words]\n",
    "    if lemmatize:\n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = pd.DataFrame(df_train['text'].apply(lambda text : clean_text(text, remove_stopwords=False)))\n",
    "df_train_cleaned[\"label\"] = df_train[\"label\"]\n",
    "df_train_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned = df_test['text'].apply(lambda text : clean_text(text, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned.to_csv(\"./data/train_cleaned.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned.to_csv(\"./data/test_cleaned.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned.apply(lambda x : len(x.split(\" \"))).sort_values(ascending=False).iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1462494    130\n",
       "1657593    109\n",
       "269709     105\n",
       "1588769    103\n",
       "965559      98\n",
       "          ... \n",
       "1694546      1\n",
       "1694545      1\n",
       "1694544      1\n",
       "1694543      1\n",
       "1694390      1\n",
       "Name: text, Length: 2500000, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned = pd.read_csv(\"./data/train_cleaned.txt\")\n",
    "df_train_cleaned.text.apply(lambda x: len(x.split(\" \"))).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 30 2022, 05:12:36) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
